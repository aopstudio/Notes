{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本概念\n",
    "## 梯度下降法和最小二乘法（正规方程）的对比\n",
    "假设有m个训练例子，n个需要训练的特征值\n",
    "### 梯度下降法\n",
    "单次迭代公式：$w:= w+\\alpha X(y-X^Tw)$\n",
    "\n",
    "* 需要选择学习率$\\alpha$\n",
    "* 需要多次迭代\n",
    "* 在n很大的时候工作的也很好\n",
    "\n",
    "### 最小二乘法\n",
    "公式：$w := (XX^T)^{-1}Xy$\n",
    "\n",
    "* 不需要选择学习率$\\alpha$\n",
    "* 不需要迭代\n",
    "* 需要计算$(XX^T)^{-1}$\n",
    "* 当n很大时运行非常慢\n",
    "\n",
    "$XX^T$不可逆的情况\n",
    "1. 存在特征线性相关，即有多余的特征\n",
    "解决方案：删除多余的特征\n",
    "2. 样本数量小于特征数量\n",
    "解决方案：删除部分特征，或者使用正则化方法\n",
    "\n",
    "## 反向传播\n",
    "将预测结果的偏差传递到各个参数上，根据这些参数对偏差的贡献的大小，相应地承担修改的责任\n",
    "## one-hot向量\n",
    "只有一个数为1，其他为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息论知识\n",
    "## 熵\n",
    "在信息论中，熵用来衡量一个随机事件的不确定性\n",
    "* 熵越高，则随机变量的信息越多\n",
    "* 熵越低，则随机变量的信息越少\n",
    "\n",
    "**自信息**：一个随机事件所包含的信息量\n",
    "\n",
    "对于一个随机变量X，当X=x时的自信息I(x)定义为\n",
    "$$I(x)=-\\log p(x)$$\n",
    "\n",
    "这个公式非常符合我们的直觉，即一件事发生的概率越低，则它包含的信息量越大\n",
    "\n",
    "### **熵：随机变量X的自信息的数学期望**\n",
    "\n",
    "对于分布律p(x)，系统的熵为\n",
    "$$H(x)=E_X[I(X)]=E_X[-\\log p(x)]=-\\sum_{x\\in X}p(x)\\log p(x)$$\n",
    "\n",
    "性质：概率分布越均衡，熵越大\n",
    "### 熵编码\n",
    "在对分布p(y)的符号进行编码时，熵H(p)也是理论上最优的平均编码长度，这种编码方式称为熵编码\n",
    "\n",
    "## 交叉熵\n",
    "交叉熵是按照概率分布q的最优编码对真实分布为p的信息进行编码的长度\n",
    "$$H(p,q)=E_p[-\\log q(x)]=-\\sum_x p(x)\\log q(x)$$\n",
    "\n",
    "在给定q的情况下，如果p和q越接近，交叉熵越小\n",
    "\n",
    "如果p和q越远，交叉熵就越大\n",
    "\n",
    "## KL散度\n",
    "KL散度使用概率分布q来近似p时所造成的的信息损失量\n",
    "\n",
    "KL散度是按照概率分布q的最优编码对真实分布为p的信息进行编码，其平均编码长度（即交叉熵）H(p,q)和p的最优平均编码长度（即熵）H(p)之间的差异\n",
    "$$KL(p,q)=H(p,q)-H(p)=\\sum_x p(x)\\log\\frac{p(x)}{q(x)}$$\n",
    "\n",
    "### 应用到机器学习\n",
    "以分类为例\n",
    "* 真实分布$p_r(y|x)$\n",
    "* 预测分布$p_\\theta(y|x)$\n",
    "\n",
    "$$KL(p_r(y|x),p_\\theta (y|x))=\\sum_y p_r(y|x)\\log\\frac{p_r(y|x)}{p_\\theta (y|x)}$$\n",
    "因为$p_r(y|x)$是确定的，所以以上式子正比于\n",
    "$$-\\sum_y p_r(y|x)\\log p_\\theta(y|x)$$\n",
    "$$=-\\log p_\\theta(y^*|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常用numpy函数\n",
    "## expand_dims\n",
    "在指定位置插入新的轴来扩展数组形状\n",
    "\n",
    "例如原本为一维的2个数据，axis=0，则shape变为(1,2),axis=1则shape变为(2,1)\n",
    "再例如 原本为 (2,3),axis=0，则shape变为(1,2,3),axis=1则shape变为(2,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15468/3001622548.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 测试expand_dims函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x.shape: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# 测试expand_dims函数\n",
    "x = np.array([1,2,3])\n",
    "print(x)\n",
    "print(\"x.shape: \",x.shape)\n",
    "y = np.expand_dims(x,axis=0)\n",
    "print(y)\n",
    "print(\"y.shape: \",y.shape)\n",
    "print(\"y[0][1]: \",y[0][1])\n",
    "y = np.expand_dims(x,axis=1)\n",
    "print(y)\n",
    "print(\"y.shape: \",y.shape)\n",
    "print(\"y[1][0]: \",y[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenate\n",
    "根据指定的维度，对一个元组、列表中的list或者ndarray进行连接\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[1 2 5]\n",
      " [3 4 6]]\n"
     ]
    }
   ],
   "source": [
    "# 例子，一个2*2的数组和一个1*2的数组，在第0维进行拼接，得到一个3*2的数组\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6]])\n",
    "c = np.concatenate((a, b), axis=0)\n",
    "print(c)\n",
    "\n",
    "# 一个2*2的数组和一个2*1的数组，在第1维进行拼接，得到一个2*3的数组：\n",
    "c = np.concatenate((a, b.T), axis=1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面两个简单的例子中，拼接的维度的长度是不同的，但是其他维度的长度必须是相同的，这也是使用concatenate()函数的一个基本原则，违背此规则就会报错，例如一个2\\*2的数组和一个1\\*2的数组，在第1维进行拼接："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码会报错\n",
    "np.concatenate((a, b), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stack\n",
    "将数组的数据按照指定的维度进行堆叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]]\n",
      "[[1 2]\n",
      " [2 3]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([2,3,4])\n",
    "# 按行堆叠\n",
    "print(np.stack([a,b],axis=0))\n",
    "# 按列堆叠\n",
    "print(np.stack([a,b],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，进行stack的两个数组必须有相同的形状，同时，输出的结果的维度是比输入的数组都要多一维的。我们拿第一个例子来举例，两个含3个数的一维数组在第0维进行堆叠，其过程等价于先给两个数组增加一个第0维，变为1\\*3的数组，再在第0维进行concatenate()操作；第二个例子则是先将两个一维数组变为3*1的二维数组，再在第1维进行concatenate()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vstack\n",
    "等同于`stack(arrays,axis=0)`\n",
    "## hstack\n",
    "等同于`stack(arrays,axis=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linalg.pinv\n",
    "求矩阵（通常是非方阵）的伪逆矩阵，X的伪逆矩阵即$(X^TX)^{-1}X^T$（易知$(X^TX)^{-1}X^TX=I$）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常用tensorflow函数\n",
    "## random.uniform\n",
    "```python\n",
    "tf.random.uniform(\n",
    "    shape, minval=0, maxval=None, dtype=tf.dtypes.float32, seed=None, name=None\n",
    ")\n",
    "```\n",
    "生成的值在[minval, maxval) 范围内遵循均匀分布\n",
    "\n",
    "## reduce_mean\n",
    "计算张量的各个维度上的元素的平均值\n",
    "```python\n",
    "reduce_mean(\n",
    "    input_tensor,\n",
    "    axis=None,\n",
    "    keep_dims=False,\n",
    "    name=None,\n",
    "    reduction_indices=None\n",
    ")\n",
    "```\n",
    "axis是tf.reduce_mean函数中的参数,按照函数中axis给定的维度减少input_tensor.除非keep_dims是true,否则张量的秩将在axis的每个条目中减少1\n",
    "例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.5, shape=(), dtype=float32)\n",
      "tf.Tensor([1.5 1.5], shape=(2,), dtype=float32)\n",
      "tf.Tensor([1. 2.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1., 1.], [2., 2.]])\n",
    "print(tf.reduce_mean(x))  # 1.5\n",
    "print(tf.reduce_mean(x, 0))  # [1.5, 1.5]\n",
    "print(tf.reduce_mean(x, 1))  # [1.,  2.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 矩阵微积分\n",
    "## 标量关于向量的偏导数\n",
    "$$\\frac{\\partial y}{\\partial \\boldsymbol{x}}=[\\frac{\\partial y}{\\partial x_1},\\dots,\\frac{\\partial y}{\\partial x_M}]^T$$\n",
    "\n",
    "## 向量关于向量的偏导数\n",
    "$$\\frac{\\partial f(\\boldsymbol{x})}{\\partial \\boldsymbol{x}}=\\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\dots  & \\frac{\\partial y_N}{\\partial x_1} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    " \\frac{\\partial y_1}{\\partial x_M} & \\dots & \\frac{\\partial y_N}{\\partial x_M}\\end{bmatrix}$$\n",
    "\n",
    "## 向量函数及其导数\n",
    "$$\\frac{\\partial \\boldsymbol{x}}{\\partial \\boldsymbol{x}} = \\boldsymbol{I}（单位矩阵）$$\n",
    "$$\\frac{\\partial \\boldsymbol{Ax}}{\\partial \\boldsymbol{x}}=\\boldsymbol{A}^T $$\n",
    "$$\\frac{\\partial \\boldsymbol{x^TA}}{\\partial \\boldsymbol{x}}=\\boldsymbol{A} \\\\$$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
