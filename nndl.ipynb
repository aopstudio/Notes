{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本概念\n",
    "## 梯度下降法和最小二乘法（正规方程）的对比\n",
    "假设有m个训练例子，n个需要训练的特征值\n",
    "### 梯度下降法\n",
    "单次迭代公式：$w:= w+\\alpha X(y-X^Tw)$\n",
    "\n",
    "* 需要选择学习率$\\alpha$\n",
    "* 需要多次迭代\n",
    "* 在n很大的时候工作的也很好\n",
    "\n",
    "### 最小二乘法\n",
    "公式：$w := (XX^T)^{-1}Xy$\n",
    "\n",
    "* 不需要选择学习率$\\alpha$\n",
    "* 不需要迭代\n",
    "* 需要计算$(XX^T)^{-1}$\n",
    "* 当n很大时运行非常慢\n",
    "\n",
    "$XX^T$不可逆的情况\n",
    "1. 存在特征线性相关，即有多余的特征\n",
    "解决方案：删除多余的特征\n",
    "2. 样本数量小于特征数量\n",
    "解决方案：删除部分特征，或者使用正则化方法\n",
    "\n",
    "## 反向传播\n",
    "将预测结果的偏差传递到各个参数上，根据这些参数对偏差的贡献的大小，相应地承担修改的责任\n",
    "## one-hot向量\n",
    "只有一个数为1，其他为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息论知识\n",
    "## 熵\n",
    "在信息论中，熵用来衡量一个随机事件的不确定性\n",
    "* 熵越高，则随机变量的信息越多\n",
    "* 熵越低，则随机变量的信息越少\n",
    "\n",
    "**自信息**：一个随机事件所包含的信息量\n",
    "\n",
    "对于一个随机变量X，当X=x时的自信息I(x)定义为\n",
    "$$I(x)=-\\log p(x)$$\n",
    "\n",
    "这个公式非常符合我们的直觉，即一件事发生的概率越低，则它包含的信息量越大\n",
    "\n",
    "### **熵：随机变量X的自信息的数学期望**\n",
    "\n",
    "对于分布律p(x)，系统的熵为\n",
    "$$H(x)=E_X[I(X)]=E_X[-\\log p(x)]=-\\sum_{x\\in X}p(x)\\log p(x)$$\n",
    "\n",
    "性质：概率分布越均衡，熵越大\n",
    "### 熵编码\n",
    "在对分布p(y)的符号进行编码时，熵H(p)也是理论上最优的平均编码长度，这种编码方式称为熵编码\n",
    "\n",
    "## 交叉熵\n",
    "交叉熵是按照概率分布q的最优编码对真实分布为p的信息进行编码的长度\n",
    "$$H(p,q)=E_p[-\\log q(x)]=-\\sum_x p(x)\\log q(x)$$\n",
    "\n",
    "在给定q的情况下，如果p和q越接近，交叉熵越小\n",
    "\n",
    "如果p和q越远，交叉熵就越大\n",
    "\n",
    "## KL散度\n",
    "KL散度使用概率分布q来近似p时所造成的的信息损失量\n",
    "\n",
    "KL散度是按照概率分布q的最优编码对真实分布为p的信息进行编码，其平均编码长度（即交叉熵）H(p,q)和p的最优平均编码长度（即熵）H(p)之间的差异\n",
    "$$KL(p,q)=H(p,q)-H(p)=\\sum_x p(x)\\log\\frac{p(x)}{q(x)}$$\n",
    "\n",
    "### 应用到机器学习\n",
    "以分类为例\n",
    "* 真实分布$p_r(y|x)$\n",
    "* 预测分布$p_\\theta(y|x)$\n",
    "\n",
    "$$KL(p_r(y|x),p_\\theta (y|x))=\\sum_y p_r(y|x)\\log\\frac{p_r(y|x)}{p_\\theta (y|x)}$$\n",
    "因为$p_r(y|x)$是确定的，所以以上式子正比于\n",
    "$$-\\sum_y p_r(y|x)\\log p_\\theta(y|x)$$\n",
    "$$=-\\log p_\\theta(y^*|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python函数\n",
    "## enumerate\n",
    "enumerate是python的内建函数，它可遍历每个元素（如列表、元组或字符串），组合为：索引 元素，常在for循环中使用\n",
    "`enumerate(列表名)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 one\n",
      "1 two\n",
      "2 three\n"
     ]
    }
   ],
   "source": [
    "seq=['one','two','three']\n",
    "for i,element in enumerate(seq):\n",
    "    print(i,element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常用numpy函数\n",
    "## expand_dims\n",
    "在指定位置插入新的轴来扩展数组形状\n",
    "\n",
    "例如原本为一维的2个数据，axis=0，则shape变为(1,2),axis=1则shape变为(2,1)\n",
    "再例如 原本为 (2,3),axis=0，则shape变为(1,2,3),axis=1则shape变为(2,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "x.shape:  (3,)\n",
      "[[1 2 3]]\n",
      "y.shape:  (1, 3)\n",
      "y[0][1]:  2\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "y.shape:  (3, 1)\n",
      "y[1][0]:  2\n"
     ]
    }
   ],
   "source": [
    "# 测试expand_dims函数\n",
    "x = np.array([1,2,3])\n",
    "print(x)\n",
    "print(\"x.shape: \",x.shape)\n",
    "y = np.expand_dims(x,axis=0)\n",
    "print(y)\n",
    "print(\"y.shape: \",y.shape)\n",
    "print(\"y[0][1]: \",y[0][1])\n",
    "y = np.expand_dims(x,axis=1)\n",
    "print(y)\n",
    "print(\"y.shape: \",y.shape)\n",
    "print(\"y[1][0]: \",y[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenate\n",
    "根据指定的维度，对一个元组、列表中的list或者ndarray进行连接\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[1 2 5]\n",
      " [3 4 6]]\n"
     ]
    }
   ],
   "source": [
    "# 例子，一个2*2的数组和一个1*2的数组，在第0维进行拼接，得到一个3*2的数组\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6]])\n",
    "c = np.concatenate((a, b), axis=0)\n",
    "print(c)\n",
    "\n",
    "# 一个2*2的数组和一个2*1的数组，在第1维进行拼接，得到一个2*3的数组：\n",
    "c = np.concatenate((a, b.T), axis=1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面两个简单的例子中，拼接的维度的长度是不同的，但是其他维度的长度必须是相同的，这也是使用concatenate()函数的一个基本原则，违背此规则就会报错，例如一个2\\*2的数组和一个1\\*2的数组，在第1维进行拼接："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码会报错\n",
    "np.concatenate((a, b), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stack\n",
    "将数组的数据按照指定的维度进行堆叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]]\n",
      "[[1 2]\n",
      " [2 3]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([2,3,4])\n",
    "# 按行堆叠\n",
    "print(np.stack([a,b],axis=0))\n",
    "# 按列堆叠\n",
    "print(np.stack([a,b],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，进行stack的两个数组必须有相同的形状，同时，输出的结果的维度是比输入的数组都要多一维的。我们拿第一个例子来举例，两个含3个数的一维数组在第0维进行堆叠，其过程等价于先给两个数组增加一个第0维，变为1\\*3的数组，再在第0维进行concatenate()操作；第二个例子则是先将两个一维数组变为3*1的二维数组，再在第1维进行concatenate()操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vstack\n",
    "等同于`stack(arrays,axis=0)`\n",
    "## hstack\n",
    "等同于`stack(arrays,axis=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linalg.pinv\n",
    "求矩阵（通常是非方阵）的伪逆矩阵，X的伪逆矩阵即$(X^TX)^{-1}X^T$（易知$(X^TX)^{-1}X^TX=I$）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow\n",
    "## 创建一个张量\n",
    "```python\n",
    "ts.constant(张量内容，dtype=数据类型（可选）)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 5], shape=(2,), dtype=int64)\n",
      "<dtype: 'int64'>\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "a=tf.constant([1,5],dtype=tf.int64)\n",
    "print(a)\n",
    "print(a.dtype)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape中看,隔开了几个数字这个张量就是几维的，隔开的数字为几则该维有几个元素\n",
    "\n",
    "## 将numpy的数据类型转换为Tensor数据类型\n",
    "```python\n",
    "tf.convert_to_tensor(数据名,dtype=数据类型（可选）)\n",
    "```\n",
    "\n",
    "## 创建全为0的张量\n",
    "`tf.zeros(维度)`\n",
    "## 创建全为1的张量\n",
    "`tf.ones(维度)`\n",
    "## 创建全为指定值的张量\n",
    "`tf.fill(维度,指定值)`\n",
    "\n",
    "维度：\n",
    "* 一维直接写个数\n",
    "* 二维用[行,列]\n",
    "* 多维用[n,m,j,k,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[9 9]\n",
      " [9 9]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a=tf.zeros([2,3])\n",
    "b=tf.ones(4)\n",
    "c=tf.fill([2,2],9)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成正态分布的随机数，默认均值为0，标准差为1\n",
    "`tf.random.normal(维度,mean=均值,stddev=标准差)`\n",
    "## 生成截断式正态分布的随机数\n",
    "`tf.random.truncated_normal(维度,mean=均值,stddev=标准差)`\n",
    "\n",
    "在truncated_normal中如果随机生成数据的取值在$(\\mu-2\\sigma,\\mu+2\\sigma)$之外，则重新进行生成，保证了生成值在均值附近\n",
    "\n",
    "## 生成均匀分布随机数 [minval,maxval)\n",
    "`tf.random.uniform(维度,minval=最小值,maxval=最大值)\n",
    "\n",
    "生成的值在[minval, maxval) 范围内遵循均匀分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.4699608  0.857422  ]\n",
      " [0.07592225 0.8647555 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "f=tf.random.uniform([2,2],minval=0,maxval=1)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 强制tensor转换为该数据类型\n",
    "`tf.cast(张量名,dtype=数据类型)`\n",
    "## 计算张量维度上元素的最小值\n",
    "`tf.reduce_min(张量名)`\n",
    "## 计算张量维度上元素的最大值\n",
    "`tf.reduce_max(张量名)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x1=tf.constant([1,2,3],dtype=tf.float64)\n",
    "print(x1)\n",
    "x2=tf.cast(x1,tf.int32)\n",
    "print(x2)\n",
    "print(tf.reduce_min(x2),tf.reduce_max(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在一个二维张量或数组中，可以通过调整axis等于0或1控制执行维度\n",
    "* axis=0代表跨行（经度，down），axis=1代表跨列（纬度，across）\n",
    "* 如果不指定axis，则所有元素参与计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 计算张量沿着指定纬度的平均值\n",
    "`tf.reduce_mean(张量名,axis=操作轴)`\n",
    "计算张量的各个维度上的元素的平均值\n",
    "\n",
    "## 计算张量沿着指定纬度的和\n",
    "`tf.reduce_sum(张量名,axis=操作轴)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [2 2 3]], shape=(2, 3), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor([2 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([3 4 6], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1,2,3], [2,2,3]])\n",
    "print(x)\n",
    "print(tf.reduce_mean(x))\n",
    "print(tf.reduce_mean(x, axis=1)) \n",
    "print(tf.reduce_sum(x, axis=0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.Variable\n",
    "ta.Variable()将变量标记为**“可训练”**，被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数\n",
    "\n",
    "`w=tf.Variable(tf.random.normal([2,2],mean=0,stddev=1))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数学运算\n",
    "* 对应元素的四则运算：tf.add, tf.substract, tf.multiply, tf.divide\n",
    "* 平方、次方与开方：tf.square, tf.pow, tf.sqrt\n",
    "* 矩阵乘：tf.matmul\n",
    "\n",
    "**只有维度相同的张量才可以做加减乘除的四则运算**\n",
    "\n",
    "## 矩阵乘tf.matmul\n",
    "`tf.matmul(矩阵1,,矩阵2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6. 6. 6.]\n",
      " [6. 6. 6.]\n",
      " [6. 6. 6.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a=tf.ones([3,2])\n",
    "b=tf.fill([2,3],3.)\n",
    "print(tf.matmul(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data.Dataset.from_tensor_slices\n",
    "切分传入张量的第一维度，生成输入特征/标签对，构建数据集\n",
    "`data=tf.data.Dataset.from_tensor_slices((输入特征,标签))`\n",
    "\n",
    "Numpy和Tensor格式都可用该语句读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=12>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=23>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=17>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "features=tf.constant([12,23,10,17])\n",
    "labels=tf.constant([0,1,1,0])\n",
    "dataset=tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "print(dataset)\n",
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.GradientTape\n",
    "with结构记录计算过程，gradient求出张量的梯度\n",
    "```python\n",
    "with tf.GradientTape() as tape:\n",
    "    若干个计算过程\n",
    "grad=tape.gradient(函数,对谁求导)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    w=tf.Variable(tf.constant(3.0))\n",
    "    loss=tf.pow(w,2)\n",
    "grad=tape.gradient(loss,w)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.one_hot\n",
    "独热编码（one-hot encoding）：在分类问题中，常用独热码做标签，标记类别\n",
    "\n",
    "1表示是，0表示非\n",
    "\n",
    "标签定义：0狗尾草鸢尾  1杂色鸢尾  2弗吉尼亚鸢尾\n",
    "当前标签：1\n",
    "独热码：（0,1,0）\n",
    "\n",
    "tf.onehot()函数将待转换数据，转换为one-hot形式的数据输出\n",
    "`tf.one_hot(待转换数据,depth=几分类)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classes=3\n",
    "labels=tf.constant([1,0,2]) #输入的元素值最小为0，最大为2\n",
    "output=tf.one_hot(labels,depth=classes)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.nn.softmax\n",
    "使用softmax函数将输出转换为符合概率分布的形式\n",
    "\n",
    "当n分类的n个输出$(y_0,y_1,...,y_{n-1})$通过softmax()函数，便符合概率分布了\n",
    "$$\\forall x\\ P(X=x)\\in [0,1] 且\\sum_x P(X=x)=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.25598174 0.69583046 0.04818781], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y=tf.constant([1.01,2.01,-0.66])\n",
    "y_pro=tf.nn.softmax(y)\n",
    "print(y_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign_sub\n",
    "用于参数的自更新\n",
    "\n",
    "赋值操作，更新参数的值并返回\n",
    "\n",
    "调用assign_sub前，先用tf.Variable定义变量w为可训练（可自更新）\n",
    "`w.assign_sub(w要自减的内容)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>\n"
     ]
    }
   ],
   "source": [
    "w=tf.Variable(4)\n",
    "w.assign_sub(1)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.argmax\n",
    "返回张量沿指定维度最大值的索引\n",
    "\n",
    "`tf.argmax(张量名,axis=操作轴)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]\n",
      " [5 4 3]\n",
      " [8 7 2]]\n",
      "tf.Tensor([3 3 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "test=np.array([[1,2,3],[2,3,4],[5,4,3],[8,7,2]])\n",
    "print(test)\n",
    "print(tf.argmax(test,axis=0)) # 返回每一列（经度）最大值的索引\n",
    "print(tf.argmax(test,axis=1)) # 返回每一行（纬度）最大值的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 矩阵微积分\n",
    "## 标量关于向量的偏导数\n",
    "$$\\frac{\\partial y}{\\partial \\boldsymbol{x}}=[\\frac{\\partial y}{\\partial x_1},\\dots,\\frac{\\partial y}{\\partial x_M}]^T$$\n",
    "\n",
    "## 向量关于向量的偏导数\n",
    "$$\\frac{\\partial f(\\boldsymbol{x})}{\\partial \\boldsymbol{x}}=\\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\dots  & \\frac{\\partial y_N}{\\partial x_1} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    " \\frac{\\partial y_1}{\\partial x_M} & \\dots & \\frac{\\partial y_N}{\\partial x_M}\\end{bmatrix}$$\n",
    "\n",
    "## 向量函数及其导数\n",
    "$$\\frac{\\partial \\boldsymbol{x}}{\\partial \\boldsymbol{x}} = \\boldsymbol{I}（单位矩阵）$$\n",
    "$$\\frac{\\partial \\boldsymbol{Ax}}{\\partial \\boldsymbol{x}}=\\boldsymbol{A}^T $$\n",
    "$$\\frac{\\partial \\boldsymbol{x^TA}}{\\partial \\boldsymbol{x}}=\\boldsymbol{A} \\\\$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 习题解答\n",
    "## 2.1 分析为什么平方损失函数不适用于分类问题\n",
    "分类问题中的标签，是没有连续的概念的。每个标签之间的距离也是没有实际意义的，所以预测值和标签两个向量之间的平方差这个值不能反应分类这个问题的优化程度。\n",
    "\n",
    "比如分类 1,2,3, 真实分类是1, 而被分类到2和3错误程度应该是一样的, 但是平方损失函数的损失却不相同\n",
    "## 2.2\n",
    "![](https://user-images.githubusercontent.com/71121922/93963870-d6422b00-fd90-11ea-9bcb-cc83874dcd33.png)\n",
    "\n",
    "$r^{(n)}$的作用：对每个样本的重视程度不一样\n",
    "## 2.3\n",
    "定理：\n",
    "1. 两个矩阵的积的秩小于等于其中任何一个矩阵的秩\n",
    "2. 矩阵的秩小于等于其行数和列数中更小的那个\n",
    "\n",
    "X是$(D+1)\\times N$的矩阵，因为$N<D+1$，所以$r(X)<=N$，而$r(X^T)=r(X)$，所以$r(XX^T)<=r(X)=N$\n",
    "\n",
    "## 2.4\n",
    "![](https://user-images.githubusercontent.com/15049049/83342556-817f2e00-a323-11ea-89ce-88123ae07518.png)\n",
    "\n",
    "## 2.5\n",
    "![](https://user-images.githubusercontent.com/15049049/83342561-9360d100-a323-11ea-9f4b-b289d40624fe.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6\n",
    "### 1)\n",
    "似然函数\n",
    "$$L(\\mu)=\\frac{1}{\\sqrt{2\\pi}\\sigma}^N \\exp(-\\sum_{i=1}^N \\frac{(x^{(i)}-\\mu)^2}{2\\sigma^2}) $$\n",
    "$$\\frac{d\\log L(\\mu)}{d\\mu}=0 $$\n",
    "$$\\frac{d(N\\log \\frac{1}{\\sqrt{2\\pi}\\sigma}-\\sum^N_{i=1}\\frac{(x^{(i)}-\\mu)^2}{2\\sigma^2})}{d\\mu}=0 $$\n",
    "$$\\sum^N_{i=1}\\frac{x^{(i)}-\\mu}{\\sigma^2}=0 $$\n",
    "$$\\sum^N_{i=1}(x^{(i)}-\\mu)=0 $$\n",
    "$$\\sum^N_{i=1}x^{(i)}=N\\mu $$\n",
    "$$\\mu=\\frac{\\sum^N_{i=1}x^{(i)}}{N} $$\n",
    "### 2)\n",
    "![](https://user-images.githubusercontent.com/15049049/83342853-9f01c700-a326-11ea-9396-5c27b0c87031.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X|\\mu)P(\\mu)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-\\frac{(X-\\mu)^2}{2\\sigma^2})\\frac{1}{\\sqrt{2\\pi}\\sigma_0}\\exp(-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2})$$\n",
    "$$L(\\mu)=\\prod_{i=1}^{N}P(x^{(i)}|\\mu)P(\\mu)=(\\frac{1}{2\\pi\\sigma\\sigma_0})^N \\exp(-\\sum_{i=1}^N\\frac{(x^{(i)}-\\mu)^2}{2\\sigma^2}-n\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2})$$\n",
    "\n",
    "$$\\frac{d\\log L(\\mu)}{d\\mu}=0$$\n",
    "$$\\frac{\\sum^N_{i=1}(x^{(i)}-\\mu)}{\\sigma^2}=\\frac{n(\\mu-\\mu_0)}{\\sigma_0^2}$$\n",
    "$$\\frac{\\sum^N_{i=1}x^{(i)}}{\\sigma^2}+\\frac{n\\mu_0}{\\sigma_0^2}=\\frac{n\\mu}{\\sigma^2}+\\frac{n\\mu}{\\sigma_0^2}$$\n",
    "$$\\mu=\\frac{\\sigma_0^2\\sum^N_{i=1}x^{(i)}+n\\sigma^2\\mu_0}{n(\\sigma^2+\\sigma_0^2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7\n",
    "根据大数定律，当N趋于无穷大时，似然函数中的正则项$n\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}$等于0"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
